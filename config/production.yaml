# Production Environment Configuration
# Optimized for production deployment

app:
  environment: "production"
  debug: false
  maintenance_mode: false

# Production API Configuration
api:
  openai:
    model: "gpt-4-turbo-preview"
    max_tokens: 4096
    timeout: 90
    max_retries: 5
  
  anthropic:
    model: "claude-3-sonnet-20240229"
    max_tokens: 4096
    timeout: 90
    max_retries: 5
  
  google:
    model: "gemini-1.5-pro"
    max_tokens: 4096
    timeout: 90
    max_retries: 5

# Production resource limits
resources:
  memory:
    max_memory_mb: 4096
    warning_threshold_mb: 3072
    cleanup_threshold_mb: 3584

  cpu:
    max_cpu_percent: 70
    max_concurrent_tasks: 8

  rate_limits:
    requests_per_minute: 100
    requests_per_hour: 2000
    requests_per_day: 20000

  storage:
    max_disk_usage_gb: 50
    log_retention_days: 90

# Conservative retry policies for production
retry:
  global:
    max_attempts: 5
    base_delay: 2.0
    max_delay: 300.0

  services:
    llm_apis:
      max_attempts: 7
      base_delay: 3.0
      max_delay: 600.0

# Production features
features:
  # Essential validation
  validation:
    enabled: true
    syntax_validation: true
    logic_validation: true
    security_validation: true
    performance_validation: true

  # Enable all integrations
  integrations:
    notion_sync: true
    github_integration: true
    monitoring: true
    analytics: true

  # Disable experimental features
  experimental:
    advanced_reasoning: false
    multi_agent_collaboration: true
    predictive_caching: true
    auto_optimization: false

# Production logging
logging:
  level: "INFO"
  format: "json"
  
  outputs:
    console:
      enabled: false
      level: "ERROR"
      format: "json"
      colorize: false
    
    file:
      enabled: true
      level: "INFO"
      format: "json"
      file_path: "logs/production.log"
      max_size_mb: 500
      backup_count: 10
    
    json_file:
      enabled: true
      level: "INFO"
      format: "json"
      file_path: "logs/production.json"
      max_size_mb: 500
      backup_count: 10

  components:
    orchestrator:
      level: "INFO"
      include_proposals: false
      include_voting: true
    
    agents:
      level: "INFO"
      include_prompts: false
      include_responses: false
    
    self_healing:
      level: "INFO"
      include_recovery_steps: true
      include_fixes: false
    
    validation:
      level: "WARNING"
      include_test_results: false

# Production security
security:
  authentication:
    required: true
    token_expiry_hours: 8
    refresh_token_enabled: true
  
  api_security:
    rate_limiting: true
    cors_enabled: true
    allowed_origins: ["https://yourdomain.com"]
    api_key_validation: true
  
  data_protection:
    encrypt_sensitive_data: true
    mask_api_keys_in_logs: true
    secure_temp_files: true
    auto_cleanup_secrets: true
  
  validation:
    validate_inputs: true
    sanitize_outputs: true
    check_malicious_patterns: true
    max_input_size_kb: 512

# Production performance
performance:
  caching:
    enabled: true
    ttl_seconds: 7200  # 2 hours
    max_cache_size_mb: 1024
    cache_hit_target: 0.9

  concurrency:
    max_concurrent_requests: 20
    worker_threads: 8
    async_enabled: true
    batch_processing: true

  optimization:
    enable_profiling: false
    memory_profiling: false
    auto_gc: true
    optimize_prompts: true

# Full monitoring in production
monitoring:
  metrics:
    enabled: true
    collection_interval: 30
    retention_days: 30
    
    performance_metrics: true
    error_metrics: true
    usage_metrics: true
    resource_metrics: true

  health_checks:
    enabled: true
    interval_seconds: 15
    timeout_seconds: 5
    endpoints:
      - name: "llm_apis"
        check_type: "api_ping"
      - name: "database"
        check_type: "db_connection"
      - name: "memory"
        check_type: "resource_usage"
      - name: "disk"
        check_type: "disk_usage"

  alerting:
    enabled: true
    webhook_url: "${ALERT_WEBHOOK_URL}"
    error_threshold: 3
    response_time_threshold_ms: 10000

# Production database settings
database:
  sqlite:
    shared_memory_db: "production_shared_memory.db"
    orchestrator_db: "production_orchestrator.db"
    agents_db: "production_agents.db"
    learning_db: "production_learning_system.db"
    recovery_db: "production_recovery_manager.db"
    notion_sync_db: "production_notion_sync.db"
    github_db: "production_github_integration.db"
    
    timeout: 60
    check_same_thread: false
    isolation_level: null
    
    page_size: 8192
    cache_size: 10000
    journal_mode: "WAL"
    synchronous: "NORMAL"

# Production-specific settings
development:
  debug:
    verbose_logging: false
    include_stack_traces: false
    profile_execution: false
    save_intermediate_results: false

  testing:
    mock_external_apis: false
    use_test_data: false
    fast_mode: false
    skip_validations: false

  hot_reload:
    enabled: false
    watch_config: false
    watch_code: false

# Production models configuration
models:
  primary_model: "claude"  # Use Claude as primary for production
  fallback_models: ["gpt", "gemini"]
  
  selection_rules:
    - condition: "task_type == 'critical'"
      model: "claude"
      reason: "Most reliable for critical tasks"
    - condition: "task_type == 'code_generation'"
      model: "gpt"
      reason: "Best for code generation"
    - condition: "task_type == 'analysis'"
      model: "claude"
      reason: "Best for detailed analysis"

# Production retry configuration
retry:
  global:
    max_attempts: 5
    base_delay: 2.0
    max_delay: 300.0
    exponential_base: 2.0
    jitter: true

  services:
    llm_apis:
      max_attempts: 7
      base_delay: 3.0
      max_delay: 600.0
      retry_on_status: [429, 500, 502, 503, 504]
      
    external_apis:
      max_attempts: 5
      base_delay: 2.0
      max_delay: 120.0
      retry_on_status: [429, 500, 502, 503, 504]
      
    database:
      max_attempts: 3
      base_delay: 1.0
      max_delay: 30.0